{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "macro-dating",
   "metadata": {},
   "source": [
    "## Analysis of Fine Tuning Runs\n",
    "\n",
    "Analysis and charts to interpret the output from the second run for \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "leading-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Add mathematical libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Graphical libraries and items.\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots, draw\n",
    "\n",
    "\n",
    "import re\n",
    "# import json\n",
    "# import datetime\n",
    "# import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "knowing-stone",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 34 fields in line 160, saw 38\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cc56b81aa28b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./runs2.log\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w266/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w266/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w266/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w266/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2054\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2056\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2057\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 34 fields in line 160, saw 38\n"
     ]
    }
   ],
   "source": [
    "### Read file into Pandas Data array\n",
    "\n",
    "file_loc = \"./runs3.log\"\n",
    "\n",
    "df = pd.read_csv(file_loc, sep='|', header=(0))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find max accuracy and min loss on training and\n",
    "### validation sets, and which epoch it was achieved on\n",
    "### This will let us see both the most accurate runs\n",
    "### and let us detect if (a) convergence has occurred\n",
    "### and (b) whether we have overfit the model\n",
    "\n",
    "epochs = 30  #output columns are counted from 0\n",
    "\n",
    "### Metrics are tuples of the metric name and + or -1\n",
    "### depending whether low or high numbers are best\n",
    "\n",
    "for metric in [ ('loss', -1), ('accuracy',1), ('val_loss',-1), ('val_accuracy',1)]:\n",
    "    best_val     = f\"{metric[0]}-best\"\n",
    "    best_epc = f\"{metric[0]}-epoch\"\n",
    "    met_sign     = metric[1]\n",
    "    \n",
    "    # Create list of the column names we want to check for the metric\n",
    "    metric_cols  = [ f\"{metric[0]}-{epoch}\" for epoch in range(0,epochs) ]\n",
    "   \n",
    "\n",
    "\n",
    "    # Find the best value for each metric, as well as the epoch in which it occurred\n",
    "    #\n",
    "    # idxmax(axis=1) returns the column name with the maximum value, idxmin does\n",
    "    # the sames for the minimum\n",
    "    #\n",
    "    # The str.extract() turns the values into strings and then pulls out only digits\n",
    "    # Ordinarily this would also pull separators like \",\" and \".\" as well, but\n",
    "    # we don't have them in the column names.\n",
    "    \n",
    "    if met_sign == 1:\n",
    "        df[best_val] = df[metric_cols].max(axis=1)\n",
    "        df[best_epc] = df[metric_cols].idxmax(axis=1).str.extract('(\\d+)').astype(int)\n",
    "    else:\n",
    "        df[best_val]     = df[metric_cols].min(axis=1)\n",
    "        df[best_epc] = df[metric_cols].idxmax(axis=1).str.extract('(\\d+)').astype(int)\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-brush",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_cols = metric_cols.copy()\n",
    "\n",
    "temp_cols.append(best_val)\n",
    "temp_cols.append(best_epc)\n",
    "# df[temp_cols]\n",
    "# type(best_val)\n",
    "\n",
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-destination",
   "metadata": {},
   "source": [
    "## Review Results by each of the hyperparameters we are varying\n",
    "\n",
    "Unless otherwise stated, we will be measuring loss and accuracy for the validation data set.\n",
    "\n",
    "\n",
    "### trainable = [True | False] - Whether we allow the embeddings to be trained as well\n",
    "\n",
    "First, do we get better accuracy by allowing the embeddings to be trained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-binding",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check best accuracy\n",
    "\n",
    "boxplot = df.boxplot(column=[\"val_accuracy-best\"], by=['train_embeds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check when the best accuracy is found in the epochs\n",
    "\n",
    "boxplot = df.boxplot(column=[\"val_accuracy-epoch\"], by=['train_embeds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-freeze",
   "metadata": {},
   "source": [
    "**From the above, better accuracy is obtained when we allow the embeddings to be refined during training, and that the model converges at least 1 epoch sooner on average.**\n",
    "The additional computational work to backpropogate into the embeddings is more than offset by the fewer epochs required to reach optimum accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Refine to only runs where we train the embeddings\n",
    "\n",
    "df1 = df[df['train_embeds'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-circulation",
   "metadata": {},
   "source": [
    "### Look at the effect of the convolutional filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = df1.boxplot(column=[\"val_accuracy-best\"], by=['kernel_sizes'], figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-hundred",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = df1.boxplot(column=[\"val_accuracy-epoch\"], by=['kernel_sizes'], figsize=(20,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-booth",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# look at each set of kernel sizes by their counts\n",
    "\n",
    "kernels = df1.kernel_sizes.unique()\n",
    "\n",
    "# create 3-wide subplots to show\n",
    "fig, ax = plt.subplots(nrows=5, ncols=2, figsize=(20, 30), sharey=True)\n",
    "\n",
    "for i in range(0,len(kernels)):\n",
    "      \n",
    "    x = i // 2\n",
    "    y = i % 2\n",
    "    \n",
    "    axis = ax[x,y]\n",
    "    \n",
    "    axis.set_ylabel(\"val_accuracy-best\")\n",
    "\n",
    "    # plt.xticks(rotation = 45) # Rotates X-Axis Ticks by 45-degrees\n",
    "    boxplot = df1[df1.kernel_sizes == kernels[i]].boxplot(column=[\"val_accuracy-best\"],\n",
    "                                                          by=['num_filters'],\n",
    "                                                          ax=axis,\n",
    "                                                          figsize=(20,10))\n",
    "    axis.title.set_text(f\"Kernel Sizes: {kernels[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-morocco",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# look at each set of filter counts by their kernel sizes\n",
    "\n",
    "filters = df1.num_filters.unique()\n",
    "\n",
    "# create 3-wide subplots to show\n",
    "# fig, ax = plt.subplots(nrows=8, ncols=2, figsize=(20, 50), sharey=True)\n",
    "fig, ax = plt.subplots(nrows=8, ncols=2, figsize=(20, 50), sharey=False)\n",
    "\n",
    "for i in range(0,len(filters)):\n",
    "      \n",
    "    x = i // 2\n",
    "    y = i % 2\n",
    "    \n",
    "    axis = ax[x,y]\n",
    "    \n",
    "    axis.set_ylabel(\"val_accuracy-best\")\n",
    "\n",
    "    # plt.xticks(rotation = 45) # Rotates X-Axis Ticks by 45-degrees\n",
    "    boxplot = df1[df1.num_filters == filters[i]].boxplot(column=[\"val_accuracy-best\"],\n",
    "                                                          by=['kernel_sizes'],\n",
    "                                                          ax=axis,\n",
    "                                                          figsize=(20,10))\n",
    "    axis.title.set_text(f\"Filter counts: {filters[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-treasure",
   "metadata": {},
   "source": [
    "**In all of the cases we tried, the best accuracies came from the highest number of filters for each set of kernels, regardless of the kernel sizes:** ```[16,32]``` for the two-filter convolutions and ```[8,16,32]``` for the three-filter ones. We should therefore test even higher counts to see if that makes any marginal improvement, including ```[32,64]``` for the two-filter convolutions, and ```[16,32,63]``` for the three-filter ones.\n",
    "\n",
    "**Equally, the largest filters generally produce the best results,** though there seems to be some fall off between ```[4,9,12]``` and ```[8,12,16]``` suggesting that a 4-word kernel does have value.  We should, in addition, test [4,8,16], [4,8,16,32] and other similar combinations to see if we can improve further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-concentration",
   "metadata": {},
   "source": [
    "## Compare dropout effect on best performing convolution\n",
    "### Kernel sizes ```[4,8,12]``` with filter counts ```[8,16,32]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1[df1[\"kernel_sizes\"] == \"[4, 8, 12]\"][df1[\"num_filters\"] == \"[8, 16, 32]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at accuracy by dropout\n",
    "boxplot = df2.boxplot(column=[\"val_accuracy-best\"], by=['dropout_rate'], figsize=(20,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-synthetic",
   "metadata": {},
   "source": [
    "**The best dropout from the sample is 0.2.** Suggest in wave 2 we search smaller steps between a 15% and 40% dropout to get the best potential output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-martial",
   "metadata": {},
   "source": [
    "## Evaluate the Dense Layers\n",
    "Look at differences in the fully connected layers within the beset selections so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2[df2[\"dropout_rate\"] == 0.2]\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at accuracy by dense layers\n",
    "\n",
    "boxplot = df3.plot.bar(x=\"dense_layer_dims\", y='val_accuracy-best', figsize=(20,10),ylim=(0.88,0.9))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-prisoner",
   "metadata": {},
   "source": [
    "**After selecting the best from the rest of the hyperparameters, the dense layers have less of an effect (within 1% of accuracy) - a single layer of only 8 nodes appears to be sufficient to give a good result.**\n",
    "\n",
    "As we have a relatively small data set for training - fewer than 7000 records in training, it may not be possible to effectively converge on larger dense-layer models.\n",
    "\n",
    "We also may want to look at f1 instead of accuracy to be certain??\n",
    "\n",
    "Round 1 testing was for 8280 tests with different hyperparameters in a total run time of 49h40m for an average of 1 test every 21.6s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test = df3[ df3[\"dense_layer_dims\"] == \"[8]\"].to_dict(orient='records')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "## print out best version:\n",
    "\n",
    "print(f\"Best validation accuracy in first hyperparameter tuning run is {best_test['val_accuracy-best']:5f}, in epoch {best_test['val_accuracy-epoch']},  Run at: {best_test['timestamp']}\")\n",
    "\n",
    "print(f\"Model:  num_filters: {best_test['num_filters']}, kernel_sizes: {best_test['kernel_sizes']},  dense_layer_dims: {best_test['dense_layer_dims']},  dropout_rate: {best_test['dropout_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-coalition",
   "metadata": {},
   "source": [
    "## TensorBoard Output\n",
    "\n",
    "Looking at the tensorboard output, it appears that although the highest accuracy was attained by epoch 9, there was clear overfitting as the loss function for the validation set started to increase after epoch 4, and the peak accuracy seems to be an outlier vs. the smoothed curve.\n",
    "\n",
    "**For future tests, we will only go to 5 epochs** unless we see evidence that more epochs might improve accuracy without overfitting.\n",
    "\n",
    "![Key](Key.png)\n",
    "![Accuracy](Accuracy.png)\n",
    "![Loss](Loss.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
